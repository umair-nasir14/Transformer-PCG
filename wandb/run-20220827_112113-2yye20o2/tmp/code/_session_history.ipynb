{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a6aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_generative as pg_nn\n",
    "from pytorch_generative import models\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import distributions\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import utils\n",
    "from torchvision.datasets import vision\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "\n",
    "from pytorch_generative import nn as pg_nn\n",
    "from pytorch_generative.models import base\n",
    "#from pytorch_generative import colab_utils\n",
    "\n",
    "from skimage import io, transform\n",
    "from skimage.util import img_as_float,img_as_int,img_as_uint\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb49661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dynamically_binarize(x):\n",
    "    return distributions.Bernoulli(probs=x).sample()\n",
    "\n",
    "\n",
    "def _resize_to_32(x):\n",
    "    return F.pad(x, (2, 2, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a12e32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<matplotlib.image.AxesImage at 0x7f4f8d8ee2f0>"
     ]
    }
   ],
   "source": [
    "plt.imshow(io.imread('/home/munasir/image-gpt/mazez/pymaze_windows/mazes/maze_0_generation.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56673eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeDataset(data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir,train=True, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            \n",
    "            root_dir (string): Directory with all the images.\n",
    "            train:\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.images = []\n",
    "        if train:\n",
    "            self.folder = 'maze_dataset/'\n",
    "            for i in range(0,2500):\n",
    "                image = Image.open('/home/munasir/image-gpt/maze_dataset/maze_'+str(i)+'_generation.png').convert('L')\n",
    "                self.images.append(image)\n",
    "        else:\n",
    "            self.folder = 'maze_dataset_test/'\n",
    "            for i in range(0,20):\n",
    "                image = Image.open('/home/munasir/image-gpt/maze_dataset_test/maze_'+str(i)+'_generation.png').convert('L')\n",
    "                self.images.append(image)\n",
    "        \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return 500#1585#len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        path_1 = '/home/munasir/image-gpt/mazez/pymaze_windows/mazes/maze_'+str(idx)+'_generation.png'\n",
    "        path_2 = os.path.join(self.root_dir,\n",
    "                                'maze_'+str(idx)+'_generation.png')\n",
    "        image = Image.open(path_1)\n",
    "        image = image.convert('RGB')\n",
    "        image = np.array(image)\n",
    "        image = np.where(image>128,[255],[0])\n",
    "        #landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        #landmarks = np.array([landmarks])\n",
    "        #landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = image\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6eb220",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/munasir/image-gpt/maze_dataset/'\n",
    "root_dir_test = '/home/munasir/image-gpt/maze_dataset_test/'\n",
    "\n",
    "\n",
    "train_set = MazeDataset(root_dir,train=True, transform=None)\n",
    "test_set = MazeDataset(root_dir_test,train=False, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57dc83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    sample = train_set[i]\n",
    "    \n",
    "\n",
    "    print(i, sample.shape,)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample,cmap='Greys')\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb2f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        landmarks = landmarks - [left, top]\n",
    "\n",
    "        return {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return torch.from_numpy(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624bc0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/munasir/image-gpt/maze_dataset/'\n",
    "root_dir_test = '/home/munasir/image-gpt/maze_dataset_test/'\n",
    "\n",
    "\n",
    "train_set = MazeDataset(root_dir,train=True, transform=transforms.Compose([ToTensor()]))\n",
    "test_set = MazeDataset(root_dir_test,train=False, transform=transforms.Compose([ToTensor()]))\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    sample = train_set[i]\n",
    "\n",
    "    print(i, sample.size())\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d95bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size, dynamically_binarize=False, resize_to_32=False):\n",
    "    \"\"\"Create train and test loaders for the MNIST dataset.\n",
    "    Args:\n",
    "        batch_size: The batch size to use.\n",
    "        dynamically_binarize: Whether to dynamically  binarize images values to {0, 1}.\n",
    "        resize_to_32: Whether to resize the images to 32x32.\n",
    "    Returns:\n",
    "        Tuple of (train_loader, test_loader).\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        #num_workers=os.cpu_count()-1,\n",
    "    )\n",
    "    test_loader = data.DataLoader(\n",
    "        test_set,\n",
    "        batch_size=batch_size,\n",
    "        #num_workers=os.cpu_count()-1,\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc65f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproduce(\n",
    "    n_epochs=457,\n",
    "    batch_size=64,\n",
    "    log_dir=\"/home/munasir/image-gpt/igpt-pytorch/pytorch-generative\",\n",
    "    n_gpus=1,\n",
    "    device_id=0,\n",
    "    debug_loader=None,\n",
    "):\n",
    "    \"\"\"Training script with defaults to reproduce results.\n",
    "    The code inside this function is self contained and can be used as a top level\n",
    "    training script, e.g. by copy/pasting it into a Jupyter notebook.\n",
    "    Args:\n",
    "        n_epochs: Number of epochs to train for.\n",
    "        batch_size: Batch size to use for training and evaluation.\n",
    "        log_dir: Directory where to log trainer state and TensorBoard summaries.\n",
    "        n_gpus: Number of GPUs to use for training the model. If 0, uses CPU.\n",
    "        device_id: The device_id of the current GPU when training on multiple GPUs.\n",
    "        debug_loader: Debug DataLoader which replaces the default training and\n",
    "            evaluation loaders if not 'None'. Do not use unless you're writing unit\n",
    "            tests.\n",
    "    \"\"\"\n",
    "    from torch import optim\n",
    "    from torch.nn import functional as F\n",
    "    from torch.optim import lr_scheduler\n",
    "\n",
    "    from pytorch_generative import datasets\n",
    "    from pytorch_generative import models\n",
    "    from pytorch_generative import trainer\n",
    "\n",
    "    train_loader, test_loader = debug_loader, debug_loader\n",
    "    if train_loader is None:\n",
    "        train_loader, test_loader = get_mnist_loaders(\n",
    "            batch_size, dynamically_binarize=False\n",
    "        )\n",
    "\n",
    "    model = models.ImageGPT(\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        in_size=64,\n",
    "        n_transformer_blocks=4,\n",
    "        n_attention_heads=4,\n",
    "        n_embedding_channels=16,\n",
    "    )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-3)#5e-3\n",
    "    scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda _: 0.999977)\n",
    "\n",
    "    def loss_fn(x, _, preds):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        batch_size = x.shape[0]\n",
    "        x, preds = x.reshape((batch_size, -1)), preds.reshape((batch_size, -1))\n",
    "        loss = F.binary_cross_entropy_with_logits(preds, x, reduction=\"none\")\n",
    "        return loss.sum(dim=1).mean()\n",
    "\n",
    "    model_trainer = trainer.Trainer(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        eval_loader=train_loader,\n",
    "        lr_scheduler=scheduler,\n",
    "        log_dir=log_dir,\n",
    "        n_gpus=n_gpus,\n",
    "        device_id=device_id,\n",
    "    )\n",
    "    \n",
    "    model_trainer.interleaved_train_and_eval(n_epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b1481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.sdk.wandb_run.Run at 0x7f4f8ce5d930>"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/munasir/image-gpt/igpt-pytorch/pytorch-generative/wandb/run-20220827_112113-2yye20o2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/evocraft_2022/Transformer-Maze-PCG/runs/2yye20o2\" target=\"_blank\">honest-feather-2</a></strong> to <a href=\"https://wandb.ai/evocraft_2022/Transformer-Maze-PCG\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Transformer-Maze-PCG\",config = {\n",
    "                    \"learning_rate\": 5e-3,\n",
    "                    \"epochs\": 100,\n",
    "                    \"batch_size\": 1,\n",
    "                    \"in_size\":64,\n",
    "                    \"n_transformer_blocks\":4,\n",
    "                    \"n_attention_heads\":4,\n",
    "                    \"n_embedding_channels\":16\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ad277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reproduce(n_epochs=1000,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eabdeddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reproduce(n_epochs=539,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aae4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in model.parameters()\n",
    ")\n",
    "print(f\"total_params: {total_params}\")\n",
    "\n",
    "trainable_params = sum(\n",
    "\tp.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"trainable_params: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc3a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_output_image(image, imsize=(100, 100)):\n",
    "  \"\"\"Reshapes the given image to (imsize, imsize, 3).\"\"\"\n",
    "  width, height = imsize\n",
    "  return image.reshape(width, height, 3).permute(2,0,1)\n",
    "\n",
    "_IMAGE_UNLOADER = transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: x.clone().squeeze(0)), transforms.ToPILImage()]\n",
    ")\n",
    "\n",
    "def imshow(batch_or_tensor, title=None, figsize=None, **kwargs):\n",
    "    \"\"\"Renders tensors as an image using Matplotlib.\n",
    "\n",
    "    Args:\n",
    "        batch_or_tensor: A batch or single tensor to render as images. If the batch size\n",
    "            > 1, the tensors are flattened into a horizontal strip before being\n",
    "            rendered.\n",
    "        title: The title for the rendered image. Passed to Matplotlib.\n",
    "        figsize: The size (in inches) for the image. Passed to Matplotlib.\n",
    "        **kwargs: Extra keyword arguments passed as pyplot.imshow(image, **kwargs).\n",
    "    \"\"\"\n",
    "    batch = batch_or_tensor\n",
    "    for _ in range(4 - batch.ndim):\n",
    "        batch = batch.unsqueeze(0)\n",
    "    n, c, h, w = batch.shape\n",
    "    tensor = batch.permute(1, 2, 0, 3).reshape(c, h, -1)\n",
    "    image = _IMAGE_UNLOADER(tensor)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2fe226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "test_image_blank = np.array(Image.new('RGB',(64,64))).transpose((2,0,1))\n",
    "test_image_blank = torch.from_numpy(test_image_blank).to('cuda:0')\n",
    "#test_image_blank = torch.to(test_image_blank.device)\n",
    "\n",
    "\n",
    "\n",
    "img = np.array(Image.open(\"/home/munasir/image-gpt/mazez/pymaze_windows/extra_mazes/maze_501_generation.png\").convert('RGB'))\n",
    "image = img.transpose((2,0,1))\n",
    "image = torch.from_numpy(image).to('cuda:0')\n",
    "\n",
    "\n",
    "\n",
    "im_crop = Image.open(\"/home/munasir/image-gpt/mazez/pymaze_windows/extra_mazes/maze_501_generation.png\").convert('RGB')\n",
    "im_crop = im_crop.crop((5,5,30,60))\n",
    "im2 = Image.new('RGB',(64,64))\n",
    "im2.paste(im_crop,box=(5,5,30,60))\n",
    "image_crop = np.array(im2)\n",
    "image_crop = np.expand_dims(image_crop,axis=0)\n",
    "image_crop = np.where(image_crop > 128,[256],[0])\n",
    "image_crop = image_crop.transpose((0,3,1,2))\n",
    "image_crop = torch.from_numpy(image_crop).to('cuda:0')\n",
    "\n",
    "def _default_sample_fn(logits):\n",
    "    return distributions.Binomial(logits=logits).sample()\n",
    "#distributions.Binomial\n",
    "#n_px_crop = 16\n",
    "#primers = img.reshape(-1,32*32)[:,:n_px_crop*32]\n",
    "#primers = primers.transpose((2,0,1))\n",
    "#primers = torch.from_numpy(primers)\n",
    "#print(primers.shape)\n",
    "r = np.random.randint(low = 50,high=1000,size=(1,))\n",
    "n_samples = 1\n",
    "with torch.no_grad():\n",
    "    for _ in range(0,n_samples):\n",
    "        o = model.forward(test_image_blank)#next(model.parameters())\n",
    "        #o = _default_sample_fn(o)\n",
    "        o = o / o.max()#Sometimes works sometimes dont\n",
    "        o = img_as_float(o.cpu().detach().numpy())#(o.cpu().detach().numpy())\n",
    "        \n",
    "        o = np.where(o<0.5,[0],[1])\n",
    "\n",
    "        print(o.max())\n",
    "        print(o.min())\n",
    "        print(o.mean())\n",
    "\n",
    "    \n",
    "        imshow(torch.tensor(o[0], dtype=torch.float32),cmap='Greys')\n",
    "        '''for rand in r:\n",
    "            o = model.forward(next(model.parameters())*rand)\n",
    "            o = o / o.max()#Sometimes works sometimes dont\n",
    "            o = img_as_float(o.cpu().detach().numpy())#(o.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "            print(o.max())\n",
    "            print(o.min())\n",
    "            print(o.mean())\n",
    "\n",
    "            o = np.where(o>0.5,[1],[0])\n",
    "            imshow(torch.tensor(o[0], dtype=torch.float32),cmap='Greys')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9747afc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reproduce(\n",
    "    n_epochs=457,\n",
    "    batch_size=64,\n",
    "    log_dir=\"/home/munasir/image-gpt/igpt-pytorch/pytorch-generative\",\n",
    "    n_gpus=1,\n",
    "    device_id=0,\n",
    "    debug_loader=None,\n",
    "):\n",
    "    \"\"\"Training script with defaults to reproduce results.\n",
    "    The code inside this function is self contained and can be used as a top level\n",
    "    training script, e.g. by copy/pasting it into a Jupyter notebook.\n",
    "    Args:\n",
    "        n_epochs: Number of epochs to train for.\n",
    "        batch_size: Batch size to use for training and evaluation.\n",
    "        log_dir: Directory where to log trainer state and TensorBoard summaries.\n",
    "        n_gpus: Number of GPUs to use for training the model. If 0, uses CPU.\n",
    "        device_id: The device_id of the current GPU when training on multiple GPUs.\n",
    "        debug_loader: Debug DataLoader which replaces the default training and\n",
    "            evaluation loaders if not 'None'. Do not use unless you're writing unit\n",
    "            tests.\n",
    "    \"\"\"\n",
    "    from torch import optim\n",
    "    from torch.nn import functional as F\n",
    "    from torch.optim import lr_scheduler\n",
    "\n",
    "    from pytorch_generative import datasets\n",
    "    from pytorch_generative import models\n",
    "    from pytorch_generative import trainer\n",
    "\n",
    "    train_loader, test_loader = debug_loader, debug_loader\n",
    "    if train_loader is None:\n",
    "        train_loader, test_loader = get_mnist_loaders(\n",
    "            batch_size, dynamically_binarize=False\n",
    "        )\n",
    "\n",
    "    model = models.ImageGPT(\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        in_size=64,\n",
    "        n_transformer_blocks=4,\n",
    "        n_attention_heads=4,\n",
    "        n_embedding_channels=8,\n",
    "    )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-3)#5e-3\n",
    "    scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda _: 0.999977)\n",
    "\n",
    "    def loss_fn(x, _, preds):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        batch_size = x.shape[0]\n",
    "        x, preds = x.reshape((batch_size, -1)), preds.reshape((batch_size, -1))\n",
    "        loss = F.binary_cross_entropy_with_logits(preds, x, reduction=\"none\")\n",
    "        return loss.sum(dim=1).mean()\n",
    "\n",
    "    model_trainer = trainer.Trainer(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        eval_loader=train_loader,\n",
    "        lr_scheduler=scheduler,\n",
    "        log_dir=log_dir,\n",
    "        n_gpus=n_gpus,\n",
    "        device_id=device_id,\n",
    "    )\n",
    "    \n",
    "    model_trainer.interleaved_train_and_eval(n_epochs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5e44ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reproduce(n_epochs=626,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cee44646",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in model.parameters()\n",
    ")\n",
    "print(f\"total_params: {total_params}\")\n",
    "\n",
    "trainable_params = sum(\n",
    "\tp.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"trainable_params: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a84fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_output_image(image, imsize=(100, 100)):\n",
    "  \"\"\"Reshapes the given image to (imsize, imsize, 3).\"\"\"\n",
    "  width, height = imsize\n",
    "  return image.reshape(width, height, 3).permute(2,0,1)\n",
    "\n",
    "_IMAGE_UNLOADER = transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: x.clone().squeeze(0)), transforms.ToPILImage()]\n",
    ")\n",
    "\n",
    "def imshow(batch_or_tensor, title=None, figsize=None, **kwargs):\n",
    "    \"\"\"Renders tensors as an image using Matplotlib.\n",
    "\n",
    "    Args:\n",
    "        batch_or_tensor: A batch or single tensor to render as images. If the batch size\n",
    "            > 1, the tensors are flattened into a horizontal strip before being\n",
    "            rendered.\n",
    "        title: The title for the rendered image. Passed to Matplotlib.\n",
    "        figsize: The size (in inches) for the image. Passed to Matplotlib.\n",
    "        **kwargs: Extra keyword arguments passed as pyplot.imshow(image, **kwargs).\n",
    "    \"\"\"\n",
    "    batch = batch_or_tensor\n",
    "    for _ in range(4 - batch.ndim):\n",
    "        batch = batch.unsqueeze(0)\n",
    "    n, c, h, w = batch.shape\n",
    "    tensor = batch.permute(1, 2, 0, 3).reshape(c, h, -1)\n",
    "    image = _IMAGE_UNLOADER(tensor)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "970393c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "test_image_blank = np.array(Image.new('RGB',(64,64))).transpose((2,0,1))\n",
    "test_image_blank = torch.from_numpy(test_image_blank).to('cuda:0')\n",
    "#test_image_blank = torch.to(test_image_blank.device)\n",
    "\n",
    "\n",
    "\n",
    "img = np.array(Image.open(\"/home/munasir/image-gpt/mazez/pymaze_windows/extra_mazes/maze_501_generation.png\").convert('RGB'))\n",
    "image = img.transpose((2,0,1))\n",
    "image = torch.from_numpy(image).to('cuda:0')\n",
    "\n",
    "\n",
    "\n",
    "im_crop = Image.open(\"/home/munasir/image-gpt/mazez/pymaze_windows/extra_mazes/maze_501_generation.png\").convert('RGB')\n",
    "im_crop = im_crop.crop((5,5,30,60))\n",
    "im2 = Image.new('RGB',(64,64))\n",
    "im2.paste(im_crop,box=(5,5,30,60))\n",
    "image_crop = np.array(im2)\n",
    "image_crop = np.expand_dims(image_crop,axis=0)\n",
    "image_crop = np.where(image_crop > 128,[256],[0])\n",
    "image_crop = image_crop.transpose((0,3,1,2))\n",
    "image_crop = torch.from_numpy(image_crop).to('cuda:0')\n",
    "\n",
    "def _default_sample_fn(logits):\n",
    "    return distributions.Binomial(logits=logits).sample()\n",
    "#distributions.Binomial\n",
    "#n_px_crop = 16\n",
    "#primers = img.reshape(-1,32*32)[:,:n_px_crop*32]\n",
    "#primers = primers.transpose((2,0,1))\n",
    "#primers = torch.from_numpy(primers)\n",
    "#print(primers.shape)\n",
    "r = np.random.randint(low = 50,high=1000,size=(1,))\n",
    "n_samples = 1\n",
    "with torch.no_grad():\n",
    "    for _ in range(0,n_samples):\n",
    "        o = model.forward(test_image_blank)#next(model.parameters())\n",
    "        #o = _default_sample_fn(o)\n",
    "        o = o / o.max()#Sometimes works sometimes dont\n",
    "        o = img_as_float(o.cpu().detach().numpy())#(o.cpu().detach().numpy())\n",
    "        \n",
    "        o = np.where(o<0.5,[0],[1])\n",
    "\n",
    "        print(o.max())\n",
    "        print(o.min())\n",
    "        print(o.mean())\n",
    "\n",
    "    \n",
    "        imshow(torch.tensor(o[0], dtype=torch.float32),cmap='Greys')\n",
    "        '''for rand in r:\n",
    "            o = model.forward(next(model.parameters())*rand)\n",
    "            o = o / o.max()#Sometimes works sometimes dont\n",
    "            o = img_as_float(o.cpu().detach().numpy())#(o.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "            print(o.max())\n",
    "            print(o.min())\n",
    "            print(o.mean())\n",
    "\n",
    "            o = np.where(o>0.5,[1],[0])\n",
    "            imshow(torch.tensor(o[0], dtype=torch.float32),cmap='Greys')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36e6e251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKbklEQVR4nO3d0XLcNgwFULuT///l7dtN2lqN6ABLSDrnMZOBxdVadziCic/X6/X6AICPj4+/dl8AAHMIBQBCKAAQQgGAEAoAhFAAIIQCACEUAIgfuy8AgFqfn59f/vuZv1W2UwAghAIAIRQACKEAQAgFAOJ099HR22wA7sNOAYAQCgCEUAAghAIAIRQACKEAQAgFAEIoABBCAYAQCgCEUAAghAIAIRQACKEAQAgFAEIoABBCAYAQCgCEUAAghAIAIRQACKEAQAgFAEIoABBCAYAQCgCEUAAghAIAIRQACKEAQAgFAEIoABBCAYD4sfsCdnm9Xkv///Pz83SNr/7v6v9fvb6rWv2sVurs+AyP1nPkyfe54h7/X53O78TKtax+J3azUwAghAIAIRQACKEAQAgFAOKx3UdHKroKqrqSKmpMWk9FB1eFqg6hzvW8u9vt6P9X1V5xhfV0fid2/E78yk4BgBAKAIRQACCEAgAhFACI091HVeeUTLGju6Wiq2BSt86Oa6no1FqtXbGezs6ZKiv1O7uMVt3tTLHdz047BQBCKAAQQgGAEAoAhFAAID5fJ1/H734jDvBuV+0YPHLmcW+nAEAIBQBCKAAQQgGAEAoAhMlrJ3VOR+uqcYVrsR7r6ahxVGdSJ9BUdgoAhFAAIIQCACEUAAgvmk/aMcTmijWq6kypUVVnSo2qOlNqUM9OAYAQCgCEUAAghAIAIRQACEN2gMeoOuZipc6kozUM2QFgiVAAIIQCACEUAAihAEA89uyj1Y6A1QEfd3K3z+Ru65lk0pCdChU/82qdm3YKAIRQACCEAgAhFAAIoQBAPLb7aMfkqIozUDrPbtkx2ct6zte58vetq8ZqnaP16Eb8yU4BgBAKAIRQACCEAgBhyM5Ddb6Yhal2DNmZxJAdAJYIBQBCKAAQQgGAEAoAxIhjLna8yfdn7f/1hDVW+uq7suMzfPJ3torP6ic7BQBCKAAQQgGAEAoAhFAAIEZ0H+04L2TSkJAVhricrzPpPBtDg2bcn9Uuoyd2dtkpABBCAYAQCgCEUAAghAIAYfIawCKT1wB4BKEAQAgFAEIoABBCAYAYcfZRJ5Ow4Jwpk+Qmqegmulqnkp0CACEUAAihAEAIBQDi9i+aq17mGHrScy2d6zmy2ggwZT1PHxo06f7cmZ0CACEUAAihAEAIBQBCKAAQI4bs6AgArmTlmTXp+WbIDgBLhAIAIRQACKEAQAgFAOL2Zx8dWe0IePewkSnXwTFDaWZzf77HTgGAEAoAhFAAIIQCACEUAIjHdh9NmT416VquPAlrynqOXHU9T7k/R57YBWinAEAIBQBCKAAQQgGAEAoAxB9PXpvU4QBQacfzrfN5aPIaAEuEAgAhFAAIoQBAPPaYi+lDduCKrvr7c4UjN951HXYKAIRQACCEAgAhFAAIoQBAPLb7aMrglNU6dxt6Yj3n61x5PRU1Otdz1W7Ejm4nOwUAQigAEEIBgBAKAIRQACAM2QE4sNpl9NUzy5AdAC5LKAAQQgGAEAoAhFAAIB579tFVzzq5spXODN6v4v74/bk+OwUAQigAEEIBgBAKAMRjXzTfbUjIVYe4HLnqetyfvhqd9+fIE1+c2ykAEEIBgBAKAIRQACCEAgDx2O6jHaZ0fRzVWe3umLSeCtbTU6OqzqSOpzuzUwAghAIAIRQACKEAQAgFAOKx3UcVXQWdg3qqzlypqDOpA6PznJtV7x4adIX1TPru8z12CgCEUAAghAIAIRQACKEAQDy2+2jHJKyqa5lSY9Kksooand0tJq/11Fit0zkZr1tFB9cZdgoAhFAAIIQCACEUAAihAEC0dR9N755YNaXb4m4dT1WmrOdu9+du6+H37BQACKEAQAgFAEIoABC3P+bCYI4/t2PoyRUGGPFfd7s/078THS/f7RQACKEAQAgFAEIoABBCAYA43X20+hZ+yp+1V72dn3JsR+eQkCsPPemqsVpn0v2Z9H3rqrFa52g9nd1uV2OnAEAIBQBCKAAQQgGAEAoAxOfr5Gv0SWeaGLYB7LTSrdTZqVTVkfYrOwUAQigAEEIBgBAKAIRQACBuP3ntyPSzTro7tXac3fLuzgzeb1KX4oqqn9l5ZtW72CkAEEIBgBAKAIRQACAe+6J5x5CQ6QNLrryeuw2ludt6umqs1ulu1Fhppph6XI+dAgAhFAAIoQBACAUAQigAEIbsQKGrdZrw/yqeTYbsAHBZQgGAEAoAhFAAIIQCAPHWs48mdRldYWAH9zF9qNMdTT+HaOq9t1MAIIQCACEUAAihAEAIBQDidPfRjjfik6Y+VUzCqrgWk73O17nyeipqTFrPlPtTVbviebi7y+iInQIAIRQACKEAQAgFAEIoABCnJ6+V/DCT12jS2R32ZJPOCppk5XMxeQ2AyxIKAIRQACCEAgBx+piLqQMhvuspQ08mrafiJVzFEQ13s+MeP+X3Z1XncR7vYqcAQAgFAEIoABBCAYAQCgDE6e6jCrvfqv/q6UNPKmpMupaKGpPuz92GOlXUmH5/jupc7agQOwUAQigAEEIBgBAKAIRQACAM2TnJEBd4nopnU+c5UYbsANBKKAAQQgGAEAoAhFAAIExe+5e7Tfa6233r7Njo7DTpvpY78ZnsZacAQAgFAEIoABBCAYA4/aK54iXPpBdFUwbBrDL05LwdQ2km1Zh0f6YPDVqtPelZVs1OAYAQCgCEUAAghAIAIRQAiNNDdqYfL9BtypCdzm4d4J92dHAZsgPAGEIBgBAKAIRQACCEAgBx+uyjTjvOF3liV8F3GUrztYr1dF3Hx8d1z+fZsZ5J50rtZqcAQAgFAEIoABBCAYAQCgCEyWtN///dta86CWtSjUlT4KbcH5PX1jqhJj3jvstOAYAQCgCEUAAghAIAIRQAiNOT10p+2IUnrwHPM72byOQ1AFoJBQBCKAAQQgGAOH3MRefgi4o/Je9++bzyM6e/nAL+qerYm4pBWrufH3YKAIRQACCEAgAhFAAIoQBAnO4+6tQ5wGfHII+nHMPR/Zn/qdXrm76eK+vsGFy5bzs6e6Z2GR2xUwAghAIAIRQACKEAQAgFAOL0kJ3pb9B1iADvUtGp1nlu3J/8TDsFAEIoABBCAYAQCgCEUAAgRpx91KmqO6riHJWKzoSqLrDO9eyYPrWyno6Ojd/VvsL9qejg6zyHaNL3bUXn73LHvbRTACCEAgAhFAAIoQBACAUAYkT3UWdHwI4zkSq6Wzq7WLp/5sq1dHYIdXffVHy3OmtUdL3s6GCqug/Tnx9Tr89OAYAQCgCEUAAghAIAMWLIzo4/XzeUB56n+3nw7gYOQ3YAaCUUAAihAEAIBQBCKAAQI465qHgLv1qjc6gGcC07huxMZacAQAgFAEIoABBCAYAQCgCEUAAghAIAIRQACKEAQAgFAEIoABBCAYAQCgCEUAAghAIAIRQACKEAQAgFAEIoABBCAYAQCgCEUAAgfuy+AAC+5/V6lde0UwAghAIAIRQACKEAQAgFAOJ091HHW24AZrFTACCEAgAhFAAIoQBACAUAQigAEEIBgBAKAIRQACD+BnqnjVP1oey8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Transformer-Maze-PCG\",config = {\n",
    "                    \"learning_rate\": 5e-3,\n",
    "                    \"epochs\": 100,\n",
    "                    \"batch_size\": 1,\n",
    "                    \"in_size\":64,\n",
    "                    \"n_transformer_blocks\":4,\n",
    "                    \"n_attention_heads\":4,\n",
    "                    \"n_embedding_channels\":8\n",
    "                    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
